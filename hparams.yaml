################################
# Experiment Parameters        #
################################
epochs: 300
iters_per_checkpoint: 500
seed: 42
dynamic_loss_scaling: True
fp16_run: False
distributed_run: False

dist_backend: "nccl"
dist_url: "tcp://localhost:54321"
cudnn_enabled: True
cudnn_benchmark: True

################################
# Data Parameters             #
################################
load_mel_from_disk: False
training_files: 'filelists/arthur_v3_train.txt'
validation_files: 'filelists/arthur_v3_valid.txt'
text_cleaners: ["english_cleaners"]  # english_cleaners, korean_cleaners
sort_by_length: False

################################
# Audio Parameters             #
################################
max_wav_value: 32768.0 # NOTE, all audio parameters the same as HiFiGAN vocoder v1 except sample rate (22050)
sampling_rate: 16000 
filter_length: 1024
hop_length: 256  # number audio of frames between stft colmns, default win_length/4
win_length: 1024  # win_length int <= n_ftt: fft window size (frequency domain), defaults to win_length: n_fft
n_mel_channels: 80 
mel_fmin: 0.0 
mel_fmax: 8000.0 

################################
# Model Parameters             #
################################
n_symbols: 80  # set 80 if u use korean_cleaners. set 65 if u use english_cleaners
symbols_embedding_dim: 512

# Transcript encoder parameters
encoder_kernel_size: 5
encoder_n_convolutions: 3
encoder_embedding_dim: 512

# Speaker embedding parameters
n_speakers: 1
speaker_embedding_dim: 16

# ---------------------------------------- # 
# emotion
n_emotions: 7  # number of emotion labels
emotion_embedding_dim: 16

# reference encoder
E: 512
ref_enc_filters: [32, 32, 64, 64, 128, 128]
ref_enc_size: [3, 3]
ref_enc_strides: [2, 2]
ref_enc_pad: [1, 1]
ref_enc_gru_size: 256 # was 512 // 2

z_latent_dim: 16
anneal_function: logistic # 'constant' is simple beta-vae
anneal_k: 0.0025
anneal_x0: 5000 # Previously 10000. This number chosen after some plotting
anneal_upper: 0.2
anneal_lag: 50000

# Prosody embedding parameters
prosody_n_convolutions: 6
prosody_conv_dim_in: [1, 32, 32, 64, 64, 128]
prosody_conv_dim_out: [32, 32, 64, 64, 128, 128]
prosody_conv_kernel: 3
prosody_conv_stride: 2
prosody_embedding_dim: 128

# Decoder parameters
n_frames_per_step: 1  # currently only 1 is supported
decoder_rnn_dim: 1024
prenet_dim: 256
max_decoder_steps: 1000
gate_threshold: 0.5
p_attention_dropout: 0.1
p_decoder_dropout: 0.1

# Attention parameters
attention_rnn_dim: 1024
attention_dim: 128

# Location Layer parameters
attention_location_n_filters: 32
attention_location_kernel_size: 31

# Mel-post processing network parameters
postnet_embedding_dim: 512
postnet_kernel_size: 5
postnet_n_convolutions: 5

################################
# Optimization Hyperparameters #
################################
use_saved_learning_rate: False
learning_rate: !!float 1e-3
weight_decay: !!float 1e-6
grad_clip_thresh: 1.0
batch_size: 16
mask_padding: True  # set model's padded outputs to padded values
